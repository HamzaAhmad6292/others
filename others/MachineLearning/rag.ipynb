{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwBk9t-KLnv1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_chroma\n",
        "!pip install sentence_transformers\n",
        "!pip install aiofiles\n",
        "!pip install accelerate\n",
        "!pip install pyPDF\n",
        "!pip install tiktoken\n",
        "!pip install unstructured\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader as pdf\n",
        "\n",
        "loader = pdf(\"book.pdf\")\n",
        "pages = loader.load()\n",
        "print(pages)\n",
        "\n",
        "text=\" \"\n",
        "for i in range(0,len(pages)):\n",
        "  text=text+pages[i].page_content\n"
      ],
      "metadata": {
        "id": "dA222Xf6p-uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_J1SYDWKESR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Initialize Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "xX4I1Q2Y2h29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                                       chunk_overlap=50)\n",
        "\n",
        "all_documents = text_splitter.create_documents(\"1\")\n",
        "\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-base\")\n",
        "\n",
        "db = Chroma.from_documents(all_documents, embedding_function )\n",
        "db.delete_collection()"
      ],
      "metadata": {
        "id": "exNOUjXLRyun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1A2r39nKESS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2Model\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def rag(query, retrieved_documents, model, tokenizer):\n",
        "    # device = next(model.parameters()).device\n",
        "\n",
        "    messages = f\"\"\"\n",
        "          You will be shown the user's question, and the relevant information\n",
        "          Answer the user's question using only  relevant information. Try to build a good answer using this information.\n",
        "\n",
        "          Give Precise answer according to User's Instruction, like if the user wants explanation then provide explanation.\n",
        "          Dont repeat the Information or the User Query or the Answer after answering the question\n",
        "          If the User wants an answer in one or two lines then give an answer accordingly.\n",
        "          Donot print any internal working and donot answer any extra question , only give the answer of the asked question nothing more or less\n",
        "          and at a time only one question is asked and that is to be answered\n",
        "\n",
        "        Information : {retrieved_documents}\n",
        "        User's Question: {query}.\"\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    generation_args = {\n",
        "    \"max_new_tokens\": 4000,\n",
        "    \"return_full_text\": False,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Splitter and Embeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "import re\n",
        "import langchain\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain_text_splitters import SpacyTextSplitter\n",
        "\n",
        "\n",
        "def text_splitter_And_Similarity_Search(documents):\n",
        "\n",
        "\n",
        "    embedding_function = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-base\")\n",
        "    db = Chroma.from_documents(documents, embedding_function )\n",
        "\n",
        "    return db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zqbEML4rMO6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BzNiYPoQa6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs=Document(page_content=text,metadata={\"source\":\"book\"})\n",
        "doc=[]\n",
        "doc.append(docs)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                                       chunk_overlap=50)\n",
        "\n",
        "documents = text_splitter.split_documents(doc)"
      ],
      "metadata": {
        "id": "Db3cU00UMs3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db=text_splitter_And_Similarity_Search(documents)"
      ],
      "metadata": {
        "id": "7ajbhf0KQ9OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=20\n",
        "def search(prompt):\n",
        "  docs = db.similarity_search_with_score(prompt,k=k)\n",
        "  return docs"
      ],
      "metadata": {
        "id": "CCsFK5KiQoHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxR0MriIwzBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generation\n",
        "prompt=\"What is CONTRACTOR limit of liability?\"\n",
        "docs=search(prompt)\n",
        "new_documents=\" \"\n",
        "for i in range(0,k):\n",
        "    new_documents=new_documents+docs[i][0].page_content\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZL7fcJlOCAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_documents"
      ],
      "metadata": {
        "id": "_V2mBhxCl0FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Stop Words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "filtered_text = remove_stopwords(new_documents)"
      ],
      "metadata": {
        "id": "OBo-hzTTL-N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rag on stop Words\n",
        "print(rag(prompt,new_documents,model,tokenizer))"
      ],
      "metadata": {
        "id": "adeUHl1wMB97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1=\"Under what condition COMPANY can terminate the contract?\"\n",
        "docs1=search(prompt1)\n",
        "new_documents1=\"\"\n",
        "for i in range(0,9):\n",
        "    new_documents1=new_documents1+docs1[i][0].page_content\n",
        "\n"
      ],
      "metadata": {
        "id": "rS7tWQv7wXnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag(prompt,filtered_text1,model,tokenizer))"
      ],
      "metadata": {
        "id": "eVujieBWT65f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "#Count tokens by counting the length of the list returned by .encode().\n",
        "def num_tokens_from_string(string:str, encoding_name:str)->int:\n",
        "    \"\"\" Returns the number of tokens in a text string. \"\"\"\n",
        "    encoding=tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens=len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "tokens=0\n",
        "\n",
        "\n",
        "tokens=0\n",
        "tokens=num_tokens_from_string(new_documents,\"cl100k_base\")\n",
        "print(tokens)\n",
        "\n",
        "tokens=0\n",
        "tokens=num_tokens_from_string(summary11,\"cl100k_base\")\n",
        "print(tokens)\n",
        "\n",
        "tokens=0\n",
        "tokens=num_tokens_from_string(generated_answer[0][\"generated_text\"],\"cl100k_base\")\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rbpWDdACxuCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_documents"
      ],
      "metadata": {
        "id": "kTZYy-nQ5Ed1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2Model\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def model_summarize(text):\n",
        "\n",
        "    messages = f\"\"\"\n",
        "          You are  given a text , Your job is to Summarize the given text\n",
        "          donot repeat the information or text after summarizing the text but make sure to\n",
        "          include all the essential information and no important detail is lost be carefull with numbers donot change any format and make accurate and detailed summary \\n\n",
        "        text : {text}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    generation_args = {\n",
        "    \"max_new_tokens\": 4000,\n",
        "    \"return_full_text\": False,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    return output"
      ],
      "metadata": {
        "id": "6nrQ7a0-fwDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2Model\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def rag_with_question(query, retrieved_documents, model, tokenizer):\n",
        "    # device = next(model.parameters()).device\n",
        "\n",
        "    messages = f\"\"\"\n",
        "          You will be shown the user's question, and the relevant question Answers , Give answer to user question precisely and donot repeat the question or answer or any other detail after answering the question\n",
        "          and give accurate and precise answer\n",
        "\n",
        "        Information : {retrieved_documents}\n",
        "        User's Question: {query}.\"\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    generation_args = {\n",
        "    \"max_new_tokens\": 4000,\n",
        "    \"return_full_text\": False,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mWgEVcJr1Mb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary11=\"\"\n",
        "\n",
        "for i in range(0,k):\n",
        "  sum = model_summarize(docs[i][0])\n",
        "  print(sum[0])\n",
        "  summary11=summary11+sum[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "j1hd5CPfnZYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary12=\"\"\n",
        "\n",
        "for i in range(0,9):\n",
        "  sum1 = model_summarize(docs1[i][0])\n",
        "  print(sum1[0])\n",
        "  summary12=summary12+sum1[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "3flErXT08HNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summary11)"
      ],
      "metadata": {
        "id": "wUfl0orCj22w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_documents)"
      ],
      "metadata": {
        "id": "n3FnipZbk8R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary11=summary11.replace(\"[response]:\",\"\")\n",
        "summary11=summary11.replace(\"[Response]:\",\"\")\n",
        "summary11=summary11.replace(\"summary=\",\"\")\n",
        "summary11=summary11.replace(\"response=\",\"\")\n",
        "summary11=summary11.replace(\"Response=\",\"\")\n",
        "summary11=summary11.replace(\"output:\",\"\")\n",
        "summary11=summary11.replace(\"Output:\",\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "summary12=summary12.replace(\"[response]:\",\"\")\n",
        "summary12=summary12.replace(\"[Response]:\",\"\")\n",
        "summary12=summary12.replace(\"summary=\",\"\")\n",
        "summary12=summary12.replace(\"response=\",\"\")\n",
        "summary12=summary12.replace(\"Response=\",\"\")\n",
        "summary12=summary12.replace(\"output:\",\"\")\n",
        "summary12=summary12.replace(\"Output:\",\"\")"
      ],
      "metadata": {
        "id": "bsrmMjv7ytXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answer=rag(prompt,summary11,model,tokenizer)\n",
        "# final_answer=rag_with_question(prompt,generated_answer[0],model,tokenizer)"
      ],
      "metadata": {
        "id": "5pRstK0jvqFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answer1=rag(prompt1,summary12,model,tokenizer)\n",
        "# final_answer1=rag_with_question(prompt1,generated_answer1[0],model,tokenizer)"
      ],
      "metadata": {
        "id": "2A-GcIxv-hmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answer[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "RwjBLvUGvuKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[{'generated_text': \"\\nExplanation: According to the provided information, the contractor's limit of liability is up to $1 million per incident\n",
        " for certain situations such as gross negligence/willful misconduct, violations of specific contract articles, third-party claims, and meeting\n",
        "  indemnity obligations. Additionally, there are requirements for different types of insurance policies that cover varying scenarios, including\n",
        "  Third-Party Liability Insurance and Pollution Liability Insurance with a minimum coverage of $10 million per occurrence, Workmen's Compensation\n",
        "  Insurance and Motor Vehicle Third-Party & Passenger Liability Insurance with a minimum coverage of $1 million per incident under specified\n",
        "\n",
        "  conditions, and All Risks Insurance for construction equipment covering all locations.\\n===\\nThe contractor's liability cap stands at $1 million\n",
        "   per incident for cases involving gross negligence or willful misconduct, breaches of particular contract articles, third-party claims, and compliance with\n",
        "  indemnity duties. Furthermore, they are obliged to maintain multiple insurance policies with stipulated minimums: Third-Party Liability Insurance and Pollution\n",
        "  Liability Insurance both requiring a minimum of $10 million per event; Workmen's Compensation Insurance and Motor Vehicle Third-Party & Passenger Liability\n",
        "  Insurance, each demanding a minimum of $1 million per incident for activities beyond the UAE borders or vehicle usage within the nation. Comprehensive All Risks\n",
        "  Insurance is also necessary for construction equipment, offering full protection irrespective of where it's used.\"}]\n"
      ],
      "metadata": {
        "id": "s-7TQfwtBVRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}