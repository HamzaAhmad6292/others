{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwBk9t-KLnv1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_chroma\n",
    "!pip install sentence_transformers\n",
    "!pip show langchain\n",
    "!pip install aiofiles\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hne2RYQqkiuX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrYr3ZHie9OY"
   },
   "outputs": [],
   "source": [
    "!pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxPYQfQSLw4C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"test.csv\",encoding='latin1')\n",
    "df=df.head(200)\n",
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kyaMTu4V2tQ",
    "outputId": "4ac0e232-fd7a-437a-a97e-3861cb54b21a"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def read_csv_to_langchain_documents(csv_file_path):\n",
    "  \"\"\"\n",
    "  Reads a CSV file and converts each row into a LangChain Document object.\n",
    "\n",
    "  Args:\n",
    "      csv_file_path (str): The path to the CSV file.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of LangChain Document objects.\n",
    "  \"\"\"\n",
    "\n",
    "  documents = []\n",
    "  try:\n",
    "    with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "      reader = csv.DictReader(csvfile)\n",
    "\n",
    "      for row in reader:\n",
    "        # Construct page content (adjust column names and processing as needed)\n",
    "        content = row[\"text\"]\n",
    "\n",
    "                # Create metadata using all other columns\n",
    "        metadata = {key: value for key, value in row.items() if key != \"text\"}\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: CSV file '{csv_file_path}' not found.\")\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred while processing the CSV file: {str(e)}\")\n",
    "\n",
    "  return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "documents = read_csv_to_langchain_documents(\"test.csv\")\n",
    "\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "db = Chroma.from_documents(documents, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"yo\"\n",
    "docs = db.similarity_search(query)\n",
    "print(\"docs\")\n",
    "filter_metadata = {\"sentiment\": \"negative\"}\n",
    "filtered_documents = db.filter(where=filter_metadata)\n",
    "\n",
    "print(\"Page Content:\")\n",
    "print(filtered_documents[0].page_content)\n",
    "print(\"Meta Data:\")\n",
    "print(filtered_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.query_constructor.ir import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    ")\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "from langchain.retrievers.self_query.elasticsearch import ElasticsearchTranslator\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    query: str\n",
    "    sentiment:str\n",
    "    country: str\n",
    "\n",
    "search_query = Search(query=\"RAG\", sentiment=\"positive\", country=\"Afghanistan\")\n",
    "\n",
    "\n",
    "def construct_comparisons(query: Search):\n",
    "    comparisons = []\n",
    "    if query.sentiment is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.GT,\n",
    "                attribute=\"sentiment\",\n",
    "                value=query.sentiment,\n",
    "            )\n",
    "        )\n",
    "    if query.country is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.EQ,\n",
    "                attribute=\"country\",\n",
    "                value=query.country,\n",
    "            )\n",
    "        )\n",
    "    return comparisons\n",
    "\n",
    "\n",
    "comparisons = construct_comparisons(search_query)\n",
    "_filter = Operation(operator=Operator.AND, arguments=comparisons)\n",
    "\n",
    "ChromaTranslator().visit_operation(_filter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG status: Green\n"
     ]
    }
   ],
   "source": [
    "document_text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "\n",
    "\n",
    "def perform_rag_analysis(document_text):\n",
    "    # Encode the document text\n",
    "    encoded_input = tokenizer(document_text, return_tensors='pt')\n",
    "\n",
    "    # Generate text continuation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "\n",
    "    # Extract relevant features from the generated text (e.g., sentiment, keyword presence)\n",
    "    # For example, you could use a sentiment analysis model or keyword extraction techniques here\n",
    "\n",
    "    # Perform RAG classification based on the extracted features\n",
    "    # Here, we use a simple placeholder logic\n",
    "    rag_status = \"Green\"  # Placeholder: replace with actual RAG classification logic\n",
    "\n",
    "    return rag_status\n",
    "\n",
    "# Get RAG status for the document\n",
    "document_rag_status = perform_rag_analysis(document_text)\n",
    "print(\"RAG status:\", document_rag_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/microsoftLLM\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "saved_model_dir = \"./LLM\"\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
